# PointNet para EstimaciÃ³n de Pose de Oil Pan

Sistema de deep learning basado en PointNet para estimar la pose (posiciÃ³n y orientaciÃ³n) de un oil pan a partir de nubes de puntos 3D.

## ğŸ¯ Objetivo

Determinar la pose de un objeto (oil pan) basÃ¡ndose en cuÃ¡nto de cada superficie o cara se estÃ¡ viendo desde un punto dado, utilizando la arquitectura PointNet adaptada para regresiÃ³n de pose 6DOF (6 grados de libertad: 3 rotaciones + 3 traslaciones).

## ğŸ“ Estructura del Proyecto

```
point_pose/
â”œâ”€â”€ data/                           # Datos del proyecto
â”‚   â”œâ”€â”€ augmented/                  # Datos aumentados
â”‚   â”œâ”€â”€ pose_dataset/              # Dataset de poses
â”‚   â””â”€â”€ processed/                 # Datos procesados
â”œâ”€â”€ checkpoints/                   # Checkpoints de entrenamiento
â”œâ”€â”€ logs/                         # Logs de entrenamiento  
â”œâ”€â”€ results/                      # Resultados de evaluaciÃ³n
â”œâ”€â”€ inference/                    # MÃ³dulo de inferencia
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ predict_pose.py           # Script de inferencia principal
â”œâ”€â”€ models/                       # Modelos y arquitecturas
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pointnet_pose.py          # Modelo principal PointNet
â”‚   â”œâ”€â”€ pointnet_utils.py         # Utilidades de PointNet
â”‚   â””â”€â”€ transform_net.py          # Redes de transformaciÃ³n T-Net
â”œâ”€â”€ training/                     # MÃ³dulo de entrenamiento
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                 # ConfiguraciÃ³n del sistema
â”‚   â”œâ”€â”€ evaluate_pose.py          # EvaluaciÃ³n del modelo
â”‚   â”œâ”€â”€ pose_loss.py             # Funciones de pÃ©rdida
â”‚   â””â”€â”€ train_pose.py            # Script de entrenamiento principal
â”œâ”€â”€ utils/                        # Utilidades generales
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ augmentation.py          # AugmentaciÃ³n de datos
â”‚   â”œâ”€â”€ data_prep_pose.py        # PreparaciÃ³n de datos
â”‚   â”œâ”€â”€ metrics.py               # MÃ©tricas de evaluaciÃ³n
â”‚   â””â”€â”€ visualization.py         # Herramientas de visualizaciÃ³n
â”œâ”€â”€ OilPan/CenteredPointClouds/  # Datos del oil pan
â”‚   â””â”€â”€ oil_pan_full_pc_10000.ply # Nube de puntos del oil pan
â””â”€â”€ README.md                     # Este archivo
```

## ğŸ› ï¸ InstalaciÃ³n

### Prerrequisitos
- Python 3.8+
- CUDA 11.0+ (recomendado para GPU)

### Dependencias
```bash
pip install torch torchvision torchaudio
pip install open3d matplotlib plotly seaborn scipy scikit-learn tqdm numpy pandas
```

### ConfiguraciÃ³n del entorno
```bash
# Clonar repositorio
git clone https://github.com/Cook131/PointNet_prueba.git
cd PointNet_prueba

# Crear archivos __init__.py necesarios
touch models/__init__.py
touch training/__init__.py
touch utils/__init__.py
touch inference/__init__.py

# Verificar estructura
ls -la models/ training/ utils/ inference/
```

## ğŸš€ Uso RÃ¡pido

### 1. Entrenamiento BÃ¡sico
```bash
# Desde la raÃ­z del proyecto
python training/train_pose.py --ply_path ./OilPan/CenteredPointClouds/oil_pan_full_pc_10000.ply
```

### 2. Entrenamiento Personalizado
```bash
python training/train_pose.py \
    --ply_path ./OilPan/CenteredPointClouds/oil_pan_full_pc_10000.ply \
    --epochs 300 \
    --batch_size 16 \
    --lr 0.0005 \
    --device cuda
```

### 3. Inferencia - OpciÃ³n 1 (desde raÃ­z)
```bash
python inference/predict_pose.py \
    --model models/best_model.pth \
    --input OilPan/CenteredPointClouds/oil_pan_full_pc_10000.ply
```

### 4. Inferencia - OpciÃ³n 2 (desde carpeta inference)
```bash
cd inference/
python predict_pose.py \
    --model ../models/best_model.pth \
    --input ../OilPan/CenteredPointClouds/oil_pan_full_pc_10000.ply
```

### 5. Inferencia sin visualizaciÃ³n
```bash
python inference/predict_pose.py \
    --model models/best_model.pth \
    --input test_oil_pan.ply \
    --no-visualize
```

### 6. Guardar resultados de inferencia
```bash
python inference/predict_pose.py \
    --model models/best_model.pth \
    --input test_oil_pan.ply \
    --output results/mi_test
```

## ğŸ“Š ConfiguraciÃ³n

El archivo `training/config.py` contiene todas las configuraciones del sistema:

### ConfiguraciÃ³n del Modelo (`ModelConfig`)
```python
input_dim: 3              # Coordenadas x, y, z
num_points: 1024          # Puntos por nube
pose_dim: 6               # 3 rotaciones + 3 traslaciones
dropout: 0.3              # Dropout rate
feature_dim: 64           # DimensiÃ³n de caracterÃ­sticas
num_pose_classes: 24      # Clases discretas de pose
```

### ConfiguraciÃ³n de Entrenamiento (`TrainingConfig`)
```python
batch_size: 32            # TamaÃ±o del batch
learning_rate: 0.001      # Learning rate inicial
num_epochs: 200           # Ã‰pocas de entrenamiento
weight_decay: 0.0005      # RegularizaciÃ³n L2
step_size: 20            # Pasos para scheduler
gamma: 0.7               # Factor de reducciÃ³n de LR
patience: 20             # Early stopping patience
```

### ConfiguraciÃ³n de Datos (`DataConfig`)
```python
train_split: 0.8          # 80% para entrenamiento
val_split: 0.1           # 10% para validaciÃ³n
test_split: 0.1          # 10% para prueba
noise_std: 0.01          # DesviaciÃ³n estÃ¡ndar del ruido
scale_range: (0.8, 1.2)  # Rango de escalado
```

### ConfiguraciÃ³n de Pose (`PoseConfig`)
```python
rotation_x_range: (-45, 45)    # Rango pitch en grados
rotation_y_range: (-45, 45)    # Rango yaw en grados  
rotation_z_range: (-180, 180)  # Rango roll en grados
rotation_step: 15.0            # Paso de discretizaciÃ³n
camera_distance: 2.0           # Distancia de cÃ¡mara
```

## ğŸ—ï¸ Arquitectura del Modelo

### PointNet Base (`models/pointnet_pose.py`)
```
Input [B, 3, N] â†’ STN3d â†’ [B, 3, N]
                â†“
          Conv1d(3â†’64) â†’ BN â†’ ReLU
                â†“
          STNkd â†’ [B, 64, N] 
                â†“
          Conv1d(64â†’128) â†’ BN â†’ ReLU
                â†“
          Conv1d(128â†’1024) â†’ BN
                â†“
          MaxPool â†’ [B, 1024]
                â†“
          FC(1024â†’512) â†’ BN â†’ ReLU â†’ Dropout
                â†“
          FC(512â†’256) â†’ BN â†’ ReLU â†’ Dropout
                â†“
          FC(256â†’6) â†’ [B, 6] (pose output)
```

### Componentes Clave

1. **Input Transform (STN3d)**: AlineaciÃ³n espacial de entrada
2. **Feature Transform (STNkd)**: TransformaciÃ³n de caracterÃ­sticas 64D
3. **Shared MLPs**: ExtracciÃ³n de caracterÃ­sticas por punto
4. **Global Feature**: Max pooling para caracterÃ­stica global
5. **Pose Head**: RegresiÃ³n a 6DOF

### Variantes Disponibles
- **PointNetPose**: RegresiÃ³n continua de pose 6DOF
- **PointNetPoseClassification**: ClasificaciÃ³n discreta de poses
- **PointNetPoseHybrid**: CombinaciÃ³n de regresiÃ³n y clasificaciÃ³n

## ğŸ“ˆ MÃ©tricas de EvaluaciÃ³n

### MÃ©tricas de PrecisiÃ³n
- **Rotation Accuracy**: % de rotaciones dentro del umbral (15Â°)
- **Translation Accuracy**: % de traslaciones dentro del umbral (0.1 unidades)
- **Combined Accuracy**: PrecisiÃ³n combinada de rotaciÃ³n y traslaciÃ³n

### MÃ©tricas de Error
- **Mean Rotation Error**: Error promedio de rotaciÃ³n en grados
- **Mean Translation Error**: Error promedio de traslaciÃ³n
- **Median Rotation Error**: Error mediano de rotaciÃ³n (mÃ¡s robusto)
- **Median Translation Error**: Error mediano de traslaciÃ³n

### EvaluaciÃ³n Completa
```bash
# Evaluar modelo entrenado
python -c "
from training.evaluate_pose import evaluate_model_complete
from models.pointnet_pose import PointNetPose
import torch

# Cargar y evaluar modelo
model = PointNetPose()
checkpoint = torch.load('models/best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])

# Ejecutar evaluaciÃ³n completa
evaluate_model_complete(model, test_loader, save_dir='results/evaluation')
"
```

## ğŸ”¬ GeneraciÃ³n de Datos SintÃ©ticos

El sistema genera automÃ¡ticamente datos de entrenamiento mediante:

### 1. **GeneraciÃ³n de Poses Aleatorias**
- Rotaciones dentro de rangos realistas para cada eje
- Traslaciones pequeÃ±as para simular variaciones de posiciÃ³n
- DistribuciÃ³n uniforme en espacios de rotaciÃ³n

### 2. **TransformaciÃ³n 3D**
- AplicaciÃ³n de matrices de rotaciÃ³n Euler XYZ
- TraslaciÃ³n en coordenadas cartesianas
- PreservaciÃ³n de la geometrÃ­a del objeto

### 3. **SimulaciÃ³n de Vista de CÃ¡mara**
- OclusiÃ³n parcial para simular vistas realistas
- EliminaciÃ³n de puntos no visibles desde la cÃ¡mara
- AdiciÃ³n de ruido para robustez

### 4. **AugmentaciÃ³n de Datos**
- Ruido gaussiano aleatorio (Ïƒ = 0.01)
- Rotaciones aleatorias menores
- Escalado aleatorio (0.8x - 1.2x)
- Jittering de puntos

## ğŸ“Š VisualizaciÃ³n y Monitoreo

### Durante el Entrenamiento
```
Epoch 50/200
Train Loss: 0.023456
Val Loss: 0.028901
Rotation Accuracy: 0.8234
Translation Accuracy: 0.7891
Combined Accuracy: 0.6945
Mean Rotation Error: 8.32Â°
Epoch Time: 45.2s
--------------------------------------------------
âœ“ Nuevo mejor modelo guardado (val_loss: 0.028901)
```

### Archivos Generados
- `logs/training_metrics.json`: MÃ©tricas completas por Ã©poca
- `logs/training_curves_epoch_X.png`: GrÃ¡ficos de progreso
- `checkpoints/checkpoint_epoch_X.pth`: Checkpoints periÃ³dicos
- `models/best_model.pth`: Mejor modelo basado en validaciÃ³n
- `results/evaluation/`: Reportes de evaluaciÃ³n completos

### VisualizaciÃ³n Manual
```python
# Desde Python
from utils.visualization import PointCloudVisualizer
import numpy as np

visualizer = PointCloudVisualizer()

# Visualizar nube de puntos
points = np.load('mi_nube_puntos.npy')
visualizer.plot_point_cloud_plotly(points, title="Mi Oil Pan")

# Comparar predicciones
true_pose = np.array([0.1, 0.2, 0.3, 0.0, 0.0, 0.0])
pred_pose = np.array([0.15, 0.18, 0.32, 0.01, -0.01, 0.005])
fig = visualizer.visualize_pose_estimation(points, true_pose, pred_pose)
fig.show()
```

## ğŸ›ï¸ Entrenamiento Avanzado

### Early Stopping
- **Paciencia**: 20 Ã©pocas sin mejora en validaciÃ³n
- **Criterio**: PÃ©rdida de validaciÃ³n (MSE + regularizaciÃ³n)
- **Guardado automÃ¡tico**: Mejor modelo segÃºn validaciÃ³n

### Learning Rate Scheduling
```python
# StepLR scheduler
initial_lr = 0.001
step_size = 20        # Reduce cada 20 Ã©pocas
gamma = 0.7          # Factor de reducciÃ³n
# LR: 0.001 â†’ 0.0007 â†’ 0.00049 â†’ ...
```

### RegularizaciÃ³n
```python
# PÃ©rdida total
total_loss = pose_loss + Î» * feature_transform_regularizer
# Î» = 0.001 (configurable)
```

### Ejemplo de Entrenamiento Completo
```bash
# Entrenamiento largo con configuraciÃ³n personalizada
python training/train_pose.py \
    --ply_path ./OilPan/CenteredPointClouds/oil_pan_full_pc_10000.ply \
    --epochs 500 \
    --batch_size 8 \
    --lr 0.0005 \
    --device cuda

# Monitorear progreso en tiempo real
tail -f logs/training_log.txt

# Evaluar durante entrenamiento
tensorboard --logdir logs/
```

## ğŸ§ª Testing y EvaluaciÃ³n

### ValidaciÃ³n de Dataset
```bash
# Verificar que el .ply se puede cargar
python -c "
from utils.data_prep_pose import validate_dataset
validate_dataset('./OilPan/CenteredPointClouds/oil_pan_full_pc_10000.ply')
"
```

### Test de Modelo
```bash
# Test rÃ¡pido del modelo
python -c "
from models.pointnet_pose import PointNetPose
import torch

model = PointNetPose()
test_input = torch.randn(1, 3, 1024)
output, transform = model(test_input)
print(f'Output shape: {output.shape}')  # Debe ser [1, 6]
print('âœ“ Modelo funciona correctamente')
"
```

### Benchmark de Rendimiento
```bash
# Medir tiempo de inferencia
python -c "
import time
from inference.predict_pose import PosePredictor

predictor = PosePredictor('models/best_model.pth')
start_time = time.time()

for i in range(100):
    pose, _ = predictor.predict_pose('test.ply', visualize=False)

avg_time = (time.time() - start_time) / 100
print(f'Tiempo promedio de inferencia: {avg_time*1000:.2f} ms')
"
```

## ğŸ”§ PersonalizaciÃ³n y ExtensiÃ³n

### Agregar Nuevos Objetos
1. **Preparar archivo .ply**:
   ```bash
   # Colocar en estructura apropiada
   mkdir OilPan/NewObject/
   cp nuevo_objeto.ply OilPan/NewObject/
   ```

2. **Actualizar configuraciÃ³n**:
   ```python
   # En training/config.py
   data_config.oil_pan_ply = "./OilPan/NewObject/nuevo_objeto.ply"
   ```

3. **Ajustar rangos de pose si es necesario**:
   ```python
   # Para objetos con diferentes orientaciones tÃ­picas
   pose_config.rotation_x_range = (-30, 30)  # Reducir rango
   pose_config.rotation_y_range = (-60, 60)  # Ampliar rango
   ```

### Modificar Arquitectura del Modelo
```python
# En models/pointnet_pose.py
class CustomPointNetPose(PointNetPose):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # Agregar capas adicionales
        self.extra_fc = nn.Linear(1024, 1024)
        self.extra_bn = nn.BatchNorm1d(1024)
        
        # Modificar cabeza final
        self.fc1 = nn.Linear(1024, 1024)  # Aumentar tamaÃ±o
        
    def forward(self, x):
        # ... cÃ³digo base ...
        
        # Procesar caracterÃ­stica global adicional
        global_feature = F.relu(self.extra_bn(self.extra_fc(global_feature)))
        
        # ... resto del forward ...
```

### MÃ©tricas Personalizadas
```python
# En utils/metrics.py o training/evaluate_pose.py
def custom_pose_metric(predictions, ground_truth):
    """
    MÃ©trica personalizada para evaluaciÃ³n especÃ­fica
    """
    # Implementar lÃ³gica personalizada
    custom_score = ...
    return custom_score

# Integrar en evaluaciÃ³n
metrics = evaluator.evaluate_pose_accuracy(pred, gt)
metrics['custom_metric'] = custom_pose_metric(pred, gt)
```

### Funciones de PÃ©rdida Personalizadas
```python
# En training/pose_loss.py
class CustomPoseLoss(nn.Module):
    def __init__(self, rotation_weight=1.0, translation_weight=1.0):
        super().__init__()
        self.rotation_weight = rotation_weight
        self.translation_weight = translation_weight
        
    def forward(self, pred_pose, true_pose):
        pred_rot, pred_trans = pred_pose[:, :3], pred_pose[:, 3:]
        true_rot, true_trans = true_pose[:, :3], true_pose[:, 3:]
        
        # PÃ©rdida de rotaciÃ³n (puede usar mÃ©tricas geodÃ©sicas)
        rot_loss = F.mse_loss(pred_rot, true_rot)
        
        # PÃ©rdida de traslaciÃ³n
        trans_loss = F.mse_loss(pred_trans, true_trans)
        
        return (self.rotation_weight * rot_loss + 
                self.translation_weight * trans_loss)
```

## ğŸ› SoluciÃ³n de Problemas

### Errores Comunes y Soluciones

#### 1. **CUDA Out of Memory**
```bash
# Error: RuntimeError: CUDA out of memory
```
**Soluciones**:
```python
# Reducir batch_size en training/config.py
training_config.batch_size = 8  # En lugar de 32

# Reducir nÃºmero de puntos si es necesario
model_config.num_points = 512   # En lugar de 1024

# Usar gradient checkpointing
torch.utils.checkpoint.checkpoint_sequential(...)
```

#### 2. **Archivo PLY No Se Carga**
```bash
# Error: OSError: Unable to read file
```
**Debugging**:
```python
# Verificar archivo
python -c "
import open3d as o3d
pcd = o3d.io.read_point_cloud('tu_archivo.ply')
print(f'Puntos cargados: {len(pcd.points)}')
print('âœ“ Archivo vÃ¡lido' if len(pcd.points) > 0 else 'âœ— Archivo invÃ¡lido')
"

# Convertir formato si es necesario
import open3d as o3d
pcd = o3d.io.read_point_cloud('input.obj')  # Otros formatos
o3d.io.write_point_cloud('output.ply', pcd)
```

#### 3. **Convergencia Lenta**
```bash
# Loss se mantiene alto despuÃ©s de muchas Ã©pocas
```
**Soluciones**:
```python
# Ajustar learning rate
training_config.learning_rate = 0.0001  # MÃ¡s conservador

# Cambiar scheduler
from torch.optim.lr_scheduler import ReduceLROnPlateau
scheduler = ReduceLROnPlateau(optimizer, patience=10, factor=0.5)

# Aumentar datos de entrenamiento
train_dataset.num_samples = 20000  # MÃ¡s datos sintÃ©ticos

# Verificar normalizaciÃ³n
points = pc_normalize(points)  # Asegurar normalizaciÃ³n correcta
```

#### 4. **Imports No Funcionan**
```bash
# Error: ModuleNotFoundError: No module named 'models'
```
**Soluciones**:
```bash
# Verificar estructura de archivos
ls models/__init__.py training/__init__.py utils/__init__.py

# Crear archivos faltantes
touch models/__init__.py
touch training/__init__.py
touch utils/__init__.py
touch inference/__init__.py

# Ejecutar desde directorio correcto
pwd  # Debe mostrar la raÃ­z del proyecto
python training/train_pose.py ...
```

#### 5. **Predicciones Incorrectas**
```bash
# El modelo predice poses muy alejadas de la realidad
```
**Debugging**:
```python
# Verificar rango de datos de entrenamiento
python -c "
import numpy as np
# Cargar datos de entrenamiento y verificar rangos
poses = np.load('training_poses.npy')
print(f'RotaciÃ³n rango: {poses[:, :3].min()} a {poses[:, :3].max()}')
print(f'TraslaciÃ³n rango: {poses[:, 3:].min()} a {poses[:, 3:].max()}')
"

# Verificar normalizaciÃ³n de entrada
points = pc_normalize(points)  # Siempre normalizar

# Verificar que el modelo estÃ© en modo eval
model.eval()
with torch.no_grad():
    prediction = model(input_tensor)
```

### Tests de ValidaciÃ³n

#### Test Completo del Sistema
```bash
# Script de test completo
python -c "
print('ğŸ§ª Testing PointNet Pose System...')

# Test 1: Imports
try:
    from models.pointnet_pose import PointNetPose
    from training.config import get_config
    from utils.data_prep_pose import validate_dataset
    print('âœ“ Imports working')
except Exception as e:
    print(f'âœ— Import error: {e}')
    exit(1)

# Test 2: ConfiguraciÃ³n
try:
    config = get_config()
    print('âœ“ Configuration loaded')
except Exception as e:
    print(f'âœ— Config error: {e}')
    exit(1)

# Test 3: Modelo
try:
    model = PointNetPose()
    test_input = torch.randn(2, 3, 1024)
    output, transform = model(test_input)
    assert output.shape == (2, 6), f'Wrong output shape: {output.shape}'
    print('âœ“ Model working')
except Exception as e:
    print(f'âœ— Model error: {e}')
    exit(1)

# Test 4: Dataset
try:
    valid = validate_dataset('./OilPan/CenteredPointClouds/oil_pan_full_pc_10000.ply')
    if valid:
        print('âœ“ Dataset valid')
    else:
        print('âœ— Dataset invalid')
except Exception as e:
    print(f'âœ— Dataset error: {e}')

print('ğŸ‰ All tests passed!')
"
```

#### Test de Entrenamiento RÃ¡pido
```bash
# Test con 1 Ã©poca y batch pequeÃ±o
python training/train_pose.py \
    --ply_path ./OilPan/CenteredPointClouds/oil_pan_full_pc_10000.ply \
    --epochs 1 \
    --batch_size 2
```

#### Test de Inferencia
```bash
# Test bÃ¡sico de inferencia
python inference/predict_pose.py \
    --model models/best_model.pth \
    --input OilPan/CenteredPointClouds/oil_pan_full_pc_10000.ply \
    --no-visualize
```

## ğŸ“š Referencias y Recursos

### Papers Fundamentales
- [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593)
- [PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](https://arxiv.org/abs/1706.02413)

### Implementaciones de Referencia
- [Repositorio Original PointNet](https://github.com/charlesq34/pointnet) - TensorFlow
- [PointNet PyTorch](https://github.com/fxia22/pointnet.pytorch) - PyTorch

### Herramientas y LibrerÃ­as
- [Open3D Documentation](http://www.open3d.org/docs/) - Procesamiento de nubes de puntos
- [PyTorch Documentation](https://pytorch.org/docs/) - Framework de deep learning
- [Plotly Python](https://plotly.com/python/) - VisualizaciÃ³n interactiva

### Datasets Relacionados
- [ModelNet40](http://modelnet.cs.princeton.edu/) - Dataset de objetos 3D
- [ShapeNet](https://www.shapenet.org/) - Dataset de formas 3D
- [ScanNet](http://www.scan-net.org/) - Dataset de escenas 3D

## ğŸ“ Licencia

MIT License - Ver archivo LICENSE para detalles.

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. **Fork** el proyecto
2. **Crear rama** para feature (`git checkout -b feature/AmazingFeature`)
3. **Commit** cambios (`git commit -m 'Add: AmazingFeature'`)
4. **Push** a la rama (`git push origin feature/AmazingFeature`)
5. **Abrir Pull Request** con descripciÃ³n detallada

### Pautas para Contribuciones
- Seguir la estructura de archivos existente
- Agregar tests para nuevas funcionalidades
- Documentar cambios en README.md
- Usar mensajes de commit descriptivos
- Mantener compatibilidad con versiones existentes

## ğŸ“ Contacto y Soporte

### Para Preguntas TÃ©cnicas
- Abrir **issue** en el repositorio con:
  - DescripciÃ³n detallada del problema
  - CÃ³digo para reproducir el error
  - InformaciÃ³n del entorno (OS, Python, CUDA)
  - Logs de error completos

### Para Sugerencias
- Abrir **feature request** con:
  - DescripciÃ³n de la funcionalidad deseada
  - Casos de uso especÃ­ficos
  - Beneficios esperados

## ğŸ”„ Changelog

### v1.0.0 (Current)
- âœ… ImplementaciÃ³n base de PointNet para estimaciÃ³n de pose
- âœ… GeneraciÃ³n automÃ¡tica de datos sintÃ©ticos
- âœ… Sistema de entrenamiento con early stopping
- âœ… MÃ©tricas de evaluaciÃ³n especÃ­ficas para pose
- âœ… VisualizaciÃ³n interactiva de resultados
- âœ… Scripts de inferencia listos para usar
- âœ… DocumentaciÃ³n completa

### Planned Features
- ğŸ”„ Soporte para mÃºltiples objetos en una escena
- ğŸ”„ IntegraciÃ³n con ROS para robÃ³tica
- ğŸ”„ ExportaciÃ³n a ONNX para deployment
- ğŸ”„ AugmentaciÃ³n de datos mÃ¡s sofisticada
- ğŸ”„ Soporte para datos RGB-D

---

**Â¡Happy coding!** ğŸš€ğŸ”§

> **Tip**: Para mejores resultados, asegÃºrate de tener al menos 10,000 samples sintÃ©ticos y entrenar por mÃ­nimo 100 Ã©pocas. Â¡La paciencia es clave en deep learning!